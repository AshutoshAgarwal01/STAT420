---
title: "Week 7 - Homework"
author: "STAT 420, Summer 2021, Ashutosh Agarwal"
date: '07/02/2021'
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---


# Directions

Students are encouraged to work together on homework. However, sharing, copying or providing any part of a homework solution or code is an infraction of the University's rules on Academic Integrity. Any violation will be punished as severely as possible.

- Be sure to remove this section if you use this `.Rmd` file as a template.
- You may leave the questions in your final document.

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

## Exercise 1 (EPA Emissions Data)

For this exercise, we will use the data stored in [`epa2017.csv`](epa2017.csv). It contains detailed descriptions of vehicles manufactured in 2017 that were used for fuel economy testing [as performed by the Environment Protection Agency]( https://www.epa.gov/air-emissions-inventories). The variables in the dataset are:  

- `Make` - Manufacturer
- `Model` - Model of vehicle
- `ID` - Manufacturer defined vehicle identification number within EPA's computer system (not a VIN number)
- `disp` - Cubic inch displacement of test vehicle
- `type` - Car, truck, or both (for vehicles that meet specifications of both car and truck, like smaller SUVs or crossovers)
- `horse` - Rated horsepower, in foot-pounds per second
- `cyl` - Number of cylinders
- `lockup` - Vehicle has transmission lockup; N or Y
- `drive` - Drivetrain system code
    - A = All-wheel drive
    - F = Front-wheel drive
    - P = Part-time 4-wheel drive
    - R = Rear-wheel drive
    - 4 = 4-wheel drive
- `weight` - Test weight, in pounds
- `axleratio` - Axle ratio
- `nvratio` - n/v ratio (engine speed versus vehicle speed at 50 mph)
- `THC` - Total hydrocarbons, in grams per mile (g/mi)
- `CO` - Carbon monoxide (a regulated pollutant), in g/mi
- `CO2` - Carbon dioxide (the primary byproduct of all fossil fuel combustion), in g/mi
- `mpg` - Fuel economy, in miles per gallon

We will attempt to model `CO2` using both `horse` and `type`. In practice, we would use many more predictors, but limiting ourselves to these two, one numeric and one factor, will allow us to create a number of plots.

Load the data, and check its structure using `str()`. Verify that `type` is a factor; if not, coerce it to be a factor.

**Answer**

- Load data

```{r}
epaData = read.csv("epa2017.csv")
epaData$typeFactor = as.factor(epaData$type)
```


**(a)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`.
- Fit a simple linear regression model with `CO2` as the response and only `horse` as the predictor.
- Add the fitted regression line to the scatterplot. Comment on how well this line models the data.
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. (Interestingly, the dataset gives the wrong drivetrain for most Subarus in this dataset, as they are almost all listed as `F`, when they are in fact all-wheel drive.)


**Answer 1(a)**

- **Training model with `CO2` as response and `horse` as predictor.**

```{r}
horseModel = lm(CO2 ~ horse, data = epaData)
```

- **Plotting `CO2` versus `horse`. Adding a fitted regression line based on the model between CO2 and horse.**

```{r}
plot(CO2 ~ horse, data = epaData, col=as.numeric(typeFactor), pch=as.numeric(typeFactor), cex = 1)
legend("bottomright", c("Both", "Car", "Truck"), col = c(1, 2, 3), pch = c(1, 2, 3))
abline(horseModel$coefficients["(Intercept)"], horseModel$coefficients["horse"])
```

This line shows trend between `horse` and `CO2` regardless of auto type. This does not look good if we see this with respect to the auto type. Trucks' and Both's emission is underestimated by this line which is not correct.

- **The average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car` is `r horseModel$coefficients["horse"]`**

Actually, average change in `CO2` per foot-pound per second increase in `horse` for **any** vehicle type is `r horseModel$coefficients["horse"]` as per this model. This is happening because our model does not depend on vehicle type.

```{r}
horseModel$coefficients["horse"]
```

- **90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`.**

Note that we have **not** included `Both` in the data or model becauase this model does not depend on type.

```{r}
predict(horseModel, newdata = data.frame(horse = 148), level = 0.9, interval = "predict")[-1]
```


**(b)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`.
- Fit an additive multiple regression model with `CO2` as the response and `horse` and `type` as the predictors.
- Add the fitted regression "lines" to the scatterplot with the same colors as their respective points (one line for each vehicle type). Comment on how well this line models the data. 
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. 

**Answer 1(b)**

- **Training additive multiple regression model with `CO2` as the response and `horse` and `type` as the predictors.**

```{r}
additiveHorseModel = lm(CO2 ~ horse + typeFactor, data = epaData)
```

- **Scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`.**

- **Adding fitted regression lines to the scatterplot, one line for each vehicle type.**

```{r}
slope = additiveHorseModel$coefficients["horse"]
carIntercept = additiveHorseModel$coefficients["(Intercept)"] + additiveHorseModel$coefficients["typeFactorCar"]
bothIntercept = additiveHorseModel$coefficients["(Intercept)"]
truckIntercept = additiveHorseModel$coefficients["(Intercept)"] + additiveHorseModel$coefficients["typeFactorTruck"]

plot(CO2 ~ horse, data = epaData, col = typeFactor, pch = as.numeric(typeFactor))
legend("bottomright", c("Both", "Car", "Truck"), col = c(1, 2, 3), pch = c(1, 2, 3))
abline(bothIntercept, slope, col = c(1), pch = c(1), lwd = 2)
abline(carIntercept, slope, col = c(2), pch = c(2), lwd = 2)
abline(truckIntercept, slope, col = c(3), pch = c(3), lwd = 2)
```

- This model will have different intercepts for each line but slope of all of them will be same.

- As per this model, difference between mean value of emission will be same for any horse power.

- These lines model data better then simple model.

- Mean for trucks is more than means for cars, which makes sense.

- However mean of trucks is still not estimated correctly. I feel that slope of truck should be steeper than it is now.

- **Average change in `CO2` for a per foot-pound per second increase in `horse` for any type of vehicle will be `r slope` as per this model. This is happening because slope of all lines is same in this model.**

```{r}
slope
```

- **90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`.**

```{r}
predict(additiveHorseModel, newdata = data.frame(horse = 148, typeFactor = "Both"), level = 0.9, interval = "predict")[-1]
```


**(c)** Do the following:

- Make a scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`. 
- Fit an interaction multiple regression model with `CO2` as the response and `horse` and `type` as the predictors.
- Add the fitted regression "lines" to the scatterplot with the same colors as their respective points (one line for each vehicle type). Comment on how well this line models the data. 
- Give an estimate for the average change in `CO2` for a one foot-pound per second increase in `horse` for a vehicle of type `car`. 
- Give a 90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`. 

**Answer 1(c)**

- **Training interaction multiple regression model with `CO2` as the response and `horse` and `type` as the predictors.**

```{r}
intHorseModel = lm(CO2 ~ horse * typeFactor, data = epaData)
intHorseModel$coefficients
```

- **Scatterplot of `CO2` versus `horse`. Use a different color point for each vehicle `type`.**

- **Adding fitted regression lines to the scatterplot, one line for each vehicle type.**

```{r}
carSlope = intHorseModel$coefficients["horse"] + intHorseModel$coefficients["horse:typeFactorCar"]
bothSlope = intHorseModel$coefficients["horse"]
truckSlope = intHorseModel$coefficients["horse"] + intHorseModel$coefficients["horse:typeFactorTruck"]

carIntercept = intHorseModel$coefficients["(Intercept)"] + additiveHorseModel$coefficients["typeFactorCar"]
bothIntercept = intHorseModel$coefficients["(Intercept)"]
truckIntercept = intHorseModel$coefficients["(Intercept)"] + additiveHorseModel$coefficients["typeFactorTruck"]

plot(CO2 ~ horse, data = epaData, col = typeFactor, pch = as.numeric(typeFactor))
legend("bottomright", c("Both", "Car", "Truck"), col = c(1, 2, 3), pch = c(1, 2, 3))
abline(bothIntercept, bothSlope, col = c(1), pch = c(1), lwd = 2)
abline(carIntercept, carSlope, col = c(2), pch = c(2), lwd = 2)
abline(truckIntercept, truckSlope, col = c(3), pch = c(3), lwd = 2)
```

- These are best models so far.
- slope of truck line is steeper than car line. This represents that trucks' emission increases at higher pace than cars' as their horse power increases.

- **Average change in `CO2` for a per foot-pound per second increase in `horse` for `car` is `r carSlope`.**

```{r}
carSlope
```

- **90% prediction interval using this model for the `CO2` of a Subaru Impreza Wagon, which is a vehicle with 148 horsepower and is considered type `Both`.**

```{r}
predict(intHorseModel, newdata = data.frame(horse = 148, typeFactor = "Both"), level = 0.9, interval = "predict")[-1]
```

**(d)** Based on the previous plots, you probably already have an opinion on the best model. Now use an ANOVA $F$-test to compare the additive and interaction models. Based on this test and a significance level of $\alpha = 0.10$, which model is preferred?

**Answer 1(d)**

- Performing ANOVA $F$-test to compare additive and interaction models. 

```{r}
an = anova(additiveHorseModel, intHorseModel)
an
```

```{r}
ifelse(an[2, "Pr(>F)"] < 0.1, "Interaction model", "Additive model")
```
- I concluded in **1(c)** that **interaction model was the best**. That conclusion is confirmed by ANOVA test at $\alpha = 0.1$

***

## Exercise 2 (Hospital SUPPORT Data, White Blood Cells)

For this exercise, we will use the data stored in [`hospital.csv`](hospital.csv). It contains a random sample of 580 seriously ill hospitalized patients from a famous study called "SUPPORT" (Study to Understand Prognoses Preferences Outcomes and Risks of Treatment). As the name suggests, the purpose of the study was to determine what factors affected or predicted outcomes, such as how long a patient remained in the hospital. The variables in the dataset are:  
 
- `Days` - Days to death or hospital discharge
- `Age` - Age on day of hospital admission
- `Sex` - Female or male
- `Comorbidity` - Patient diagnosed with more than one chronic disease
- `EdYears` - Years of education
- `Education` - Education level; high or low
- `Income` - Income level; high or low
- `Charges` - Hospital charges, in dollars
- `Care` - Level of care required; high or low
- `Race` - Non-white or white
- `Pressure` - Blood pressure, in mmHg
- `Blood` - White blood cell count, in gm/dL
- `Rate` - Heart rate, in bpm

For this exercise, we will use `Age`, `Education`, `Income`, and `Sex` in an attempt to model `Blood`. Essentially, we are attempting to model white blood cell count using only demographic information.

**(a)** Load the data, and check its structure using `str()`. Verify that `Education`, `Income`, and `Sex` are factors; if not, coerce them to be factors. What are the levels of `Education`, `Income`, and `Sex`?

**Answer 2(a)**

- **Loading, exploring and preparing data**

```{r}
# Loading
hospitalData = read.csv("hospital.csv")

# Explore data.
str(hospitalData)
```

- **`Education`, `Income` and `Sex` are `chr` variables. We need to convert these to factors.**

```{r}
hospitalData$Education = as.factor(hospitalData$Education)
hospitalData$Income = as.factor(hospitalData$Income)
hospitalData$Sex = as.factor(hospitalData$Sex)
```

- **Levels of `Education` variable**

```{r}
levels(hospitalData$Education)
```


- **Levels of `Income` variable**

```{r}
levels(hospitalData$Income)
```


- **Levels of `Sex` variable**

```{r}
levels(hospitalData$Sex)
```

**(b)** Fit an additive multiple regression model with `Blood` as the response using `Age`, `Education`, `Income`, and `Sex` as predictors. What does `R` choose as the reference level for `Education`, `Income`, and `Sex`?

**Answer 2(b) This model will look like below.**

  $Y$ = $\beta_0$ + $\beta_1 * x_1$ + $\beta_2 * x_2$ + $\beta_3 * x_3$ + $\beta_4 * x_4$ + $\epsilon$

```{r}
additiveHosp = lm(Blood ~ Age + Education + Income + Sex, data = hospitalData)
additiveHosp
```

- In this model, reference level for discreet variables chosen by R are below. `R` has selected first level (when sorted alphabetically in ascending order) as reference.
  - `Education` - High
  - `Income` - High
  - `Sex` - Female.

**(c)** Fit a multiple regression model with `Blood` as the response. Use the main effects of `Age`, `Education`, `Income`, and `Sex`, as well as the interaction of `Sex` with `Age` and the interaction of `Sex` and `Income`. Use a statistical test to compare this model to the additive model using a significance level of $\alpha = 0.10$. Which do you prefer?

**Answer (2c)** 

  - Using 2-level interaction between desired terms and then using remaining terms as stand alone terms
    
    Note that `Sex * Age` and `Sex * Income` will cover variables `Sex`, `Age` and `Income`. We will simply need to add last variable `Education` as stand alone.
  
```{r}
shortHospModel = lm(Blood ~ Education +  Sex * Age + Sex * Income, data = hospitalData)
shortHospModel
```

  - Statistical test to compare this model to the additive model using a significance level of $\alpha = 0.10$.
  
    Since additive model is sub-model of interaction model, we can use `ANOVA` to perform `F-test`
    
```{r}
an = anova(additiveHosp, shortHospModel)
ifelse(an[2, "Pr(>F)"] < 0.1, "Interaction model", "Additive model")
```
  
  **We `Fail to reject` null hypothesis. i.e. we select `Additive model`**

**(d)** Fit a model similar to that in **(c)**, but additionally add the interaction between `Income` and `Age` as well as a three-way interaction between `Age`, `Income`, and `Sex`. Use a statistical test to compare this model to the preferred model from **(c)** using a significance level of $\alpha = 0.10$. Which do you prefer?

**Answer (2d)** This model is going to contain all terms of three way model between `Age`, `Income` and `Sex` plus a standalone term for `Education`.
  
```{r}
veryLargeHospModel = lm(Blood ~ Education + Sex * Income * Age, data = hospitalData)
```
  
  - We preferred `Additive model` from **c**. Comparing that model with our new large model.
    Testing model for significance level $\alpha = 0.1$

```{r}
an = anova(additiveHosp, veryLargeHospModel)
ifelse(an[2, "Pr(>F)"] < 0.1, "Very large Interaction model", "Additive model")
```
    
  **In this case, we prefer very large interactive model. i.e. model created in (d)**

**(e)** Using the model in **(d)**, give an estimate of the change in average `Blood` for a one-unit increase in `Age` for a highly educated, low income, male patient.

**Answer (e)**
  
```{r}
veryLargeHospModel$coefficients["Age"] + veryLargeHospModel$coefficients["Sexmale:Age"] + veryLargeHospModel$coefficients["Incomelow:Age"] + veryLargeHospModel$coefficients["Sexmale:Incomelow:Age"]
```

***

## Exercise 3 (Hospital SUPPORT Data, Stay Duration)

For this exercise, we will again use the data stored in [`hospital.csv`](hospital.csv). It contains a random sample of 580 seriously ill hospitalized patients from a famous study called "SUPPORT" (Study to Understand Prognoses Preferences Outcomes and Risks of Treatment). As the name suggests, the purpose of the study was to determine what factors affected or predicted outcomes, such as how long a patient remained in the hospital. The variables in the dataset are:  
 
- `Days` - Days to death or hospital discharge
- `Age` - Age on day of hospital admission
- `Sex` - Female or male
- `Comorbidity` - Patient diagnosed with more than one chronic disease
- `EdYears` - Years of education
- `Education` - Education level; high or low
- `Income` - Income level; high or low
- `Charges` - Hospital charges, in dollars
- `Care` - Level of care required; high or low
- `Race` - Non-white or white
- `Pressure` - Blood pressure, in mmHg
- `Blood` - White blood cell count, in gm/dL
- `Rate` - Heart rate, in bpm

For this exercise, we will use `Blood`, `Pressure`, and `Rate` in an attempt to model `Days`. Essentially, we are attempting to model the time spent in the hospital using only health metrics measured at the hospital.

Consider the model

\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon,
\]

where

- $Y$ is `Days`
- $x_1$ is `Blood`
- $x_2$ is `Pressure`
- $x_3$ is `Rate`.

**(a)** Fit the model above. Also fit a smaller model using the provided `R` code.

```{r, eval = FALSE}
days_add = lm(Days ~ Pressure + Blood + Rate, data = hospital)
```

Use a statistical test to compare the two models. Report the following:

- The null and alternative hypotheses in terms of the model given in the exercise description
- The value of the test statistic
- The p-value of the test
- A statistical decision using a significance level of $\alpha = 0.10$
- Which model you prefer

**Answer 3(a)** 

  - Fitting model provided in exercise.
  
    This model contains all first order (individual variables), second order (2-way interactions) and third order terms (three-way interactions). Therefore we can create a full 3-way interaction model to achive that.
    
```{r}
days_int = lm(Days ~ Pressure * Blood * Rate, data = hospitalData)
```
    
  - Fitting additive model.
  
```{r}
days_add = lm(Days ~ Pressure + Blood + Rate, data = hospitalData)
```
  
  - **Null Hypothesis**: $H_0: \beta_4 = \beta_5 = \beta_6 = \beta_7 = 0$
    
      $Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon$

  - **Alternative Hypothesis**: $H_1:$ At-least one of $\beta_4, \beta_5, \beta_6, \beta_7$ is not zero.
    
      $Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon$
      
  - **Comparing models** Model `days_add` is a sub-model of `days_int` therefore we can perform `F-test` using `ANOVA`.
    
```{r}
an = anova(days_add, days_int)
```

  - **Test statistic (F-Statistic):** `r an$F[2]`
      
  - **P-Value:** `r an[2, "Pr(>F)"]`
      
  - **Statistical decision at $\alpha = 0.1$**
      
```{r}
ifelse(an[2, "Pr(>F)"] < 0.1, "Reject null hypothesis (i.e. we select interaction model)", "Fail to reject null hypothesis (i.e. we select additive model")
```
        
**(b)** Give an expression based on the model in the exercise description for the true change in length of hospital stay in days for a 1 bpm increase in `Rate` for a patient with a `Pressure` of 139 mmHg and a `Blood` of 10 gm/dL. Your answer should be a linear function of the $\beta$s.

**Answer (3b)**

  - We are basically asked to tell expression for slope of `Rate` for given values of `Pressure` and `Blood`
  
    Since $x_3$ represents `Rate`, we will re-arrange equation around that: $Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_4 x_1 x_2 + (\beta_3 + \beta_5 x_1 + \beta_6 x_2 + \beta_7 x_1 x_2) x_3 + \epsilon$
    
    Replacing values `Pressure` = 139 mmHg and a `Blood` = 10 gm/dL we get:
    
    $\beta_3 + \beta_5 * 10 + \beta_6 * 139 + \beta_7 * 10 * 139$
    
    Absolute value will be:
    
```{r}
days_int$coefficients[4] + days_int$coefficients[6] * 10 + days_int$coefficients[7] * 139 + days_int$coefficients[8] * 10 * 139
```
    
**(c)** Give an expression based on the additive model in part **(a)** for the true change in length of hospital stay in days for a 1 bpm increase in `Rate` for a patient with a `Pressure` of 139 mmHg and a `Blood` of 10 gm/dL. Your answer should be a linear function of the $\beta$s.

**Answer (3c)**

  - It's same as **3b** but for additive model this time.
    
  - Let's define equation for additive model first.
  
    $Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon$
    
  - Since $x_3$ represents rate, **$\beta_3$** will be true change in length of hospital stay in days for 1 bpm increase in `Rate` for a patient. Since it does not depend upon other variables in this model, it's value is going to be constant for any given value of `Pressure` and `Blood`
  
  - It's value will be: `r days_add$coefficients["Rate"]`

***

## Exercise 4 ($t$-test Is a Linear Model)

In this exercise, we will try to convince ourselves that a two-sample $t$-test assuming equal variance is the same as a $t$-test for the coefficient in front of a single two-level factor variable (dummy variable) in a linear model.

First, we set up the data frame that we will use throughout.

```{r}
n = 30

sim_data = data.frame(
  groups = c(rep("A", n / 2), rep("B", n / 2)),
  values = rep(0, n))
str(sim_data)
```

We will use a total sample size of `30`, `15` for each group. The `groups` variable splits the data into two groups, `A` and `B`, which will be the grouping variable for the $t$-test and a factor variable in a regression. The `values` variable will store simulated data.

We will repeat the following process a number of times.

```{r}
set.seed(20)
sim_data$values = rnorm(n, mean = 42, sd = 3.5) # simulate response data
summary(lm(values ~ groups, data = sim_data))
t.test(values ~ groups, data = sim_data, var.equal = TRUE)
```

We use `lm()` to test

\[
H_0: \beta_1 = 0
\]

for the model

\[
Y = \beta_0 + \beta_1 x_1 + \epsilon
\]

where $Y$ is the values of interest, and $x_1$ is a dummy variable that splits the data in two. We will let `R` take care of the dummy variable.

We use `t.test()` to test

\[
H_0: \mu_A = \mu_B
\]

where $\mu_A$ is the mean for the `A` group, and $\mu_B$ is the mean for the `B` group.

The following code sets up some variables for storage.

```{r}
num_sims = 300
lm_t = rep(0, num_sims)
lm_p = rep(0, num_sims)
tt_t = rep(0, num_sims)
tt_t_g = rep(0, num_sims)
tt_t_l = rep(0, num_sims)
tt_p = rep(0, num_sims)
```

- `lm_t` will store the test statistic for the test $H_0: \beta_1 = 0$.
- `lm_p` will store the p-value for the test $H_0: \beta_1 = 0$.
- `tt_t` will store the test statistic for the test $H_0: \mu_A = \mu_B$.
- `tt_p` will store the p-value for the test $H_0: \mu_A = \mu_B$.

The variable `num_sims` controls how many times we will repeat this process, which we have chosen to be `300`.

**(a)** Set a seed equal to your birthday. Then write code that repeats the above process `300` times. Each time, store the appropriate values in `lm_t`, `lm_p`, `tt_t`, and `tt_p`. Specifically, each time you should use `sim_data$values = rnorm(n, mean = 42, sd = 3.5)` to update the data. The grouping will always stay the same.

**Answer 4(a)**

  - Setting seed
```{r}
set.seed(18691002)
```

  - Writing loop to calculate metrics on simulated data.
  
```{r}
for (i in 1:num_sims)
{
  # Simulate new response.
  sim_data$values = rnorm(n, mean = 42, sd = 3.5)
  
  # Fit linear model
  model = lm(values ~ groups, data = sim_data)
  
  # t-test
  ttest = t.test(values ~ groups, data = sim_data, var.equal = TRUE)
  
  # populate metrics.
  lm_t[i] = summary(model)$coefficients["groupsB", "t value"]
  lm_p[i] = summary(model)$coefficients["groupsB", "Pr(>|t|)"]
  tt_t[i] = ttest$statistic
  tt_p[i] = ttest$p.value
}
```

**(b)** Report the value obtained by running `mean(lm_t == tt_t)`, which tells us what proportion of the test statistics is equal. The result may be extremely surprising!

**Answer 4b**

```{r}
mean(lm_t == tt_t)
```
No value of `lm_t` is equal to `tt_t`. This is unreal :)

**(c)** Report the value obtained by running `mean(lm_p == tt_p)`, which tells us what proportion of the p-values is equal. The result may be extremely surprising!

**Answer 4c**

```{r}
mean(lm_p == tt_p)
```

**(d)** If you have done everything correctly so far, your answers to the last two parts won't indicate the equivalence we want to show! What the heck is going on here? The first issue is one of using a computer to do calculations. When a computer checks for equality, it demands **equality**; nothing can be different. However, when a computer performs calculations, it can only do so with a certain level of precision. So, if we calculate two quantities we know to be analytically equal, they can differ numerically. Instead of `mean(lm_p == tt_p)` run `all.equal(lm_p, tt_p)`. This will perform a similar calculation, but with a very small error tolerance for each equality. What is the result of running this code? What does it mean?

**Answer 4d**

  - Proportion of the test statistics that are equal. Calculated using `all.equal`
  
```{r}
all.equal(lm_t, tt_t)
```
This is still not telling us that `lm_t` and `tt_t` are equal. We will explore this in **4e**.

  - Proportion of the p-values that are equal. Calculated using `all.equal`
  
```{r}
all.equal(lm_p, tt_p)
```
This tells that p-values different very small, this could be because of precision.

**(e)** Your answer in **(d)** should now make much more sense. Then what is going on with the test statistics? Look at the values stored in `lm_t` and `tt_t`. What do you notice? Is there a relationship between the two? Can you explain why this is happening?

**Answer 4e**

  - We noticed in **4d** that sign of test statistics in linear model and two sample t-test are opposite. Let's print some data and see what's going on.
  
```{r}
head(data.frame(lm_t, tt_t))
```
  
  - Notice that their signs are opposite but their absolute values are same. Let's calculate proportion of the test statistics that are equal using `all.equal` with `abs` of both values.
  
```{r}
all.equal(abs(lm_t), abs(tt_t))
```

  - Note that now they are all equal. This tells us that test statistics in t-test and linear model are of opposite sign.
  
  - It could be because `lm` and `t.test` have used different reference for `group` variable. We know from `summary(lm)` that `groupA` is used as reference. But `t.test` may be using `groupB` as reference. How to prove that?