---
title: "Week 4 - Homework"
author: "STAT 420, Summer 2021, Ashutosh Agarwal (aa61)"
date: '06/14/2021'
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---


# Directions

Students are encouraged to work together on homework. However, sharing, copying or providing any part of a homework solution or code is an infraction of the University's rules on Academic Integrity. Any violation will be punished as severely as possible.

- Be sure to remove this section if you use this `.Rmd` file as a template.
- You may leave the questions in your final document.

***

## Exercise 1 (Using `lm`)

For this exercise we will use the data stored in [`nutrition-2018.csv`](nutrition-2018.csv). It contains the nutritional values per serving size for a large variety of foods as calculated by the USDA in 2018. It is a cleaned version totaling 5956 observations and is current as of April 2018.

The variables in the dataset are:

- `ID` 
- `Desc` - short description of food
- `Water` - in grams
- `Calories` 
- `Protein` - in grams
- `Fat` - in grams
- `Carbs` - carbohydrates, in grams
- `Fiber` - in grams
- `Sugar` - in grams
- `Calcium` - in milligrams
- `Potassium` - in milligrams
- `Sodium` - in milligrams
- `VitaminC` - vitamin C, in milligrams
- `Chol` - cholesterol, in milligrams
- `Portion` - description of standard serving size used in analysis

**(a)** Fit the following multiple linear regression model in `R`. Use `Calories` as the response and `Fat`, `Sugar`, and `Sodium` as predictors.

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i.
\]

Here,

- $Y_i$ is `Calories`.
- $x_{i1}$ is `Fat`.
- $x_{i2}$ is `Sugar`.
- $x_{i3}$ is `Sodium`.

Use an $F$-test to test the significance of the regression. Report the following:
 
- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

**Answer**

```{r}
nutrition = read.csv("nutrition-2018.csv")
n_model = lm(Calories ~ Fat + Sugar + Sodium, data = nutrition)
```

- **Null hypothesis:** $H_0$: $\beta_1 = \beta_2 = \beta_3 = 0$ i.e.None of the predictors Fat, Sugar and Sodium have significant linear relationship with response Calories.

- **Alternative hypothesis:** $H_1$: At least one of $\beta_j \neq 0, j = 0, 1, 2$ i.e. At-least one of the predictors Fat, Sugar or Sodium have significant linear relationship with response Calories.

- **Test statistic (F-Statistic):** `r summary(n_model)$f["value"]`

Since we have MLR in hand with multiple predictors. Test statistic used will be F-statistic

```{r}
fstat = summary(n_model)$f
fstat["value"]
```

- **p-value:** `r pf(summary(n_model)$fstatistic[1L], summary(n_model)$fstatistic[2L], summary(n_model)$fstatistic[3L], lower.tail = FALSE)`

```{r}
# see stats:::print.summary.lm
# pval = format.pval(pf(summary(n_model)$fstatistic[1L], summary(n_model)$fstatistic[2L], summary(n_model)$fstatistic[3L], lower.tail = FALSE))
pval = pf(summary(n_model)$fstatistic[1L], summary(n_model)$fstatistic[2L], summary(n_model)$fstatistic[3L], lower.tail = FALSE)
pval
```

Note: p-value is coming out the model as 0. But I believe that it is really small value that R is printing as 0.

- **A statistical decision at $\alpha = 0.01$** Since 0.01 is larger than p-value (`r pval`) we can reject null hypothesis. This suggests that at-least one of the predictors have significant linear relationship with response.

- **A conclusion in the context of the problem** p-value (`r pval`) is very small. Therefore for any reasonable value of $\alpha$ we can reject null hypothesis. In other words, at-least one of the predictors Fat, Sugar or Sodium have significant linear relationship with response Calories.

**(b)** Output only the estimated regression coefficients. Interpret all $\hat{\beta}_j$ coefficients in the context of the problem.

**Answer**

```{r}
Interpretation = c("Average calories in a serving of food when Fat, Sugar and Sodium are zero.", "Average change in Calories for unit increase in Fat for given values of Sugar and Sodium", "Average change in Calories for unit increase in Sugar for given values of Fat and Sodium", "Average change in Calories for unit increase in Sodium for given values of Fat and Sugar")

Interpretation = cbind(Interpretation, coef(n_model))

library(knitr)
kable(Interpretation)
```


**(c)** Use your model to predict the number of `Calories` in a Filet-O-Fish. According to [McDonald's publicized nutrition facts](https://www.mcdonalds.com/us/en-us/about-our-food/nutrition-calculator.html), the Filet-O-Fish contains 18g of fat, 5g of sugar, and 580mg of sodium.

**Answer**

```{r}
newdata = data.frame(cbind(18, 5, 580))
colnames(newdata) = names(coef(n_model))[-1]
predict(n_model, newdata = newdata)
```

**(d)** Calculate the standard deviation, $s_y$, for the observed values in the Calories variable. Report the value of $s_e$ from your multiple regression model. Interpret both estimates in the context of this problem.

**Answer**

```{r}
sd = sd(nutrition$Calories)
sd
sy = sd(predict(n_model, newdata = subset(nutrition, select =  c("Fat", "Sugar", "Sodium"))))
sy
se = summary(n_model)$sigma
se
```

  - Standard deviation of observed values of calories is `r sd`. This tells spread of actual values.
  - Standard deviation (**$s_y$**) of predicted values of calories is `r sy`. This tells spread of predicted values.
  - Standard error of model (**$s_e$**) from the MLR is `r se`. This tells spread of difference between actual values and predicted values.


**(e)** Report the value of $R^2$ for the model. Interpret its meaning in the context of the problem.

**Answer**

```{r}
summary(n_model)$r.squared
```

  - Value of $R^2$ is `r summary(n_model)$r.squared`.
  - This tells strength of linear regression between Calories and predictors (Fat, Sugar and Sodium)

**(f)** Calculate a 90% confidence interval for $\beta_2$. Give an interpretation of the interval in the context of the problem.

**Answer**

```{r}
n = length(nutrition$Calories)
p = 4
crit = qt(0.95, df = n - p, lower.tail = TRUE)

# Note that we have used std error of model, not beta_2
# se = summary(n_model)$coefficients["Sugar", "Std. Error"]
se = summary(n_model)$sigma
est = summary(n_model)$coefficients["Sugar", "Estimate"]

# Create a matrix with first column as all ones. This column is added for intercepts.
X = data.matrix(cbind(rep(1, n), subset(nutrition, select = c("Fat", "Sugar", "Sodium"))))
covmat = solve(t(X) %*% X)

# C33, because beta_2's covar is stored in C33
C33 = covmat[3,3]

interval = c(est - crit * se * sqrt(C33), est + crit * se * sqrt(C33))
interval
```

Same could be done using `confint` as well.

```{r}
confint(n_model, parm = "Sugar", level = 0.90, interval = c("confidence"))
```

**Interpretation:** We can say with 90% confidence that average change in Calories for unit increase in Sugar for given values of Fat and Sodium can vary between `r interval[1]` and `r interval[2]`

**(g)** Calculate a 95% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

**Answer**

```{r}
interval = confint(n_model, parm = "(Intercept)", level = 0.95, interval = c("confidence"))
interval
```

**Interpretation:** We can say with 95% confidence that average value of Calories per serving size can vary between `r interval[1]` and `r interval[2]` when values of Sugar, Fat and Sodium are zero.

**(h)** Use a 99% confidence interval to estimate the mean Calorie content of a food with 15g of fat, 0g of sugar, and 260mg of sodium, which is true of a medium order of McDonald's french fries. Interpret the interval in context.
 
**Answer**

```{r}
n = length(nutrition$Calories)
p = 4
crit = qt(0.995, df = n - p, lower.tail = TRUE)
se = summary(n_model)$sigma
newdata = data.frame(Fat = c(15), Sugar = c(0), Sodium = c(260))
est = predict(n_model, newdata = newdata)

# Create a matrix with first column as all ones. This column is added for intercepts.
X = data.matrix(cbind(rep(1, n), subset(nutrition, select = c("Fat", "Sugar", "Sodium"))))
covmat = solve(t(X) %*% X)

x0 = data.matrix(cbind(1, newdata))
term = x0 %*% covmat %*% t(x0)

interval = c(est - crit * se * sqrt(term), est + crit * se * sqrt(term))
interval
``` 
 
Same could be done using `predict` as well.

```{r}
predict(n_model, newdata = newdata, level = 0.99, interval = c("confidence"))
```

**Interpretation:** We can say with 99% confidence that average value of Calories per serving size for food with 15g of fat, 0g of sugar, and 260mg of sodium  can vary between `r interval[1]` and `r interval[2]`
 
**(i)** Use a 99% prediction interval to predict the Calorie content of a Crunchy Taco Supreme, which has 11g of fat, 2g of sugar, and 340mg of sodium according to [Taco Bell's publicized nutrition information](https://www.tacobell.com/nutrition/info). Interpret the interval in context.

**Answer**:

```{r}
newdata = data.frame(Fat = c(11), Sugar = c(2), Sodium = c(340))
interval = predict(n_model, newdata = newdata, level = 0.99, interval = c("predict"))
interval
```

**Interpretation:** We can say with 99% confidence that value of Calories per serving size for food with 11g of fat, 2g of sugar, and 340mg of sodium  can vary between `r interval[2]` and `r interval[3]`. We can see that lower bound is negative which is not possible in real world. This happened because given data point was not within range of given data set.

***

## Exercise 2 (More `lm` for Multiple Regression)

For this exercise we will use the data stored in [`goalies17.csv`](goalies17.csv). It contains career data for goaltenders in the National Hockey League during the first 100 years of the league from the 1917-1918 season to the 2016-2017 season. It holds the 750 individuals who played at least one game as goalie over this timeframe. The variables in the dataset are:
 
- `Player` - Player's Name (those followed by * are in the Hall of Fame as of 2017)
- `First` - First year with game recorded as goalie
- `Last` - Last year with game recorded as goalie
- `Active` - Number of seasons active in the NHL
- `GP` - Games Played
- `GS` - Games Started
- `W` - Wins
- `L` - Losses (in regulation)
- `TOL` - Ties and Overtime Losses
- `GA` - Goals Against
- `SA` - Shots Against
- `SV` - Saves
- `SV_PCT` - Save Percentage
- `GAA` - Goals Against Average
- `SO` - Shutouts
- `PIM` - Penalties in Minutes
- `MIN` - Minutes

For this exercise we will consider three models, each with Wins as the response. The predictors for these models are:

- Model 1: Goals Against, Saves
- Model 2: Goals Against, Saves, Shots Against, Minutes, Shutouts
- Model 3: All Available

After reading in the data but prior to any modeling, you should clean the data set for this exercise by removing the following variables: `Player`, `GS`, `L`, `TOL`, `SV_PCT`, and `GAA`.

**Data loading and cleaning**:

```{r}
nhl = read.csv("goalies17.csv")
nhl = subset(nhl, select = -c(Player, GS, L, TOL, SV_PCT, GAA))
```

**Creating models**

```{r}
model1 = lm(W ~ GA + SV, data = nhl)
model2 = lm(W ~ GA + SV + SA + MIN + SO, data = nhl)
model3 = lm(W ~ ., data = nhl)
```

**Setting common variables**

```{r}
n = length(nhl$W)
p1 = length(model1$coefficients)
p2 = length(model2$coefficients)
p3 = length(model3$coefficients)
```

**(a)** Use an $F$-test to compares Models 1 and 2. Report the following:

- The null hypothesis
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- The model you prefer

**Answer**

- **Comparing using ANOVA:**

```{r}
an = anova(model1, model2)
```

- **Null Hypothesis:** $H_0: \beta_{SA} = \beta_{MIN} = \beta_{SO} = 0$

- **Test statistic (F-Statistic):** `r an$F[2]`

```{r}
an$F[2]
```

- **P-value:** `r an["Pr(>F)"][2,1]` is very small. Therefore for any reasonable value of $\alpha$ we can reject null hypothesis.

- **Note:** Actual pvalue in the summary was "2.2e-16" however is is extracted as different value. 

```{r}
an["Pr(>F)"][2,1]
```

- **A statistical decision at $\alpha = 0.05$:** 0.05 is very large compared to p-value (`r an["Pr(>F)"][2,1]`). Therefore we can reject null hypothesis at $\alpha = 0.05$.

In other words, at-least one of predictors SA, MIN and SO has significant linear relationship with outcome.

- **Preferred model:** I would prefer `model2` that has predictors "Goals Against, Saves, Shots Against, Minutes, Shutouts" against `model1` that has predictors GA and SV because p-value of comparison is extremely low.

**(b)** Use an $F$-test to compare Model 3 to your preferred model from part **(a)**. Report the following:

- The null hypothesis
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- The model you prefer

**Answer**

- **Comparing using ANOVA:**

```{r}
an = anova(model2, model3)
```

- **Null Hypothesis:** $H_0: \beta_{First} = \beta_{Last} = \beta_{Active} = \beta_{GP} = \beta_{PIM} = 0$

- **Test statistic (F-Statistic):** `r an$F[2]`

```{r}
an$F[2]
```

- **P-value:** `r an["Pr(>F)"][2,1]` is very small. Therefore for any reasonable value of $\alpha$ we can reject null hypothesis.

```{r}
an["Pr(>F)"][2,1]
```

- **A statistical decision at $\alpha = 0.05$:** 0.05 is very large compared to p-value (`r an["Pr(>F)"][2,1]`). Therefore we can reject null hypothesis at $\alpha = 0.05$.

In other words, at-least one of predictors not used in model2 has significant linear relationship with outcome.

- **Preferred model:** I would prefer `model3` that has all predictors against `model2` because p-value of the F-test is extremely low.

**(c)** Use a $t$-test to test $H_0: \beta_{\texttt{SV}} = 0 \ \text{vs} \ H_1: \beta_{\texttt{SV}} \neq 0$ for the model you preferred in part **(b)**. Report the following:

- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$

**Answer**

We preferred `model3` in part **(b)**

- **Test statistic (t-statistic):** Since we are testing for only one parameter we will be using t-test.

```{r}
summary(model3)$coefficients["SV", "t value"]
```

- **The p-value of the test:** 

```{r}
pval = summary(model3)$coefficients["SV", "Pr(>|t|)"]
pval
```
- **A statistical decision at $\alpha = 0.05$:** Since 0.05 is very large compared to `pvalue = ` `r pval`. We can reject null hypothesis. In other words Given other predictors, SV has significant linear relationship with response.

***

## Exercise 3 (Regression without `lm`)

For this exercise we will once again use the `Ozone` data from the `mlbench` package. The goal of this exercise is to fit a model with `ozone` as the response and the remaining variables as predictors.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Obtain the estimated regression coefficients **without** the use of `lm()` or any other built-in functions for regression. That is, you should use only matrix operations. Store the results in a vector `beta_hat_no_lm`. To ensure this is a vector, you may need to use `as.vector()`. Return this vector as well as the results of `sum(beta_hat_no_lm ^ 2)`.

**Answer**

Calculating regression coefficients. $\hat\beta = (X^TX)^{-1}X^Ty$

```{r}
predictors = subset(Ozone, select = c("wind", "humidity", "temp"))
response = data.matrix(subset(Ozone, select = c("ozone")))

# add a column to accommodate for intercept.
X = data.matrix(cbind(rep(1, nrow(Ozone)), predictors))

beta_hat_no_lm = solve(t(X) %*% X) %*% t(X) %*% response
beta_hat_no_lm = as.vector(beta_hat_no_lm)
```

Coefficients **without** using `lm()` `r beta_hat_no_lm`

`sum(beta_hat_no_lm ^ 2)` = `r sum(beta_hat_no_lm ^ 2)`


**(b)** Obtain the estimated regression coefficients **with** the use of `lm()`. Store the results in a vector `beta_hat_lm`. To ensure this is a vector, you may need to use `as.vector()`. Return this vector as well as the results of `sum(beta_hat_lm ^ 2)`.

**Answer**

```{r}
omodel = lm(ozone ~ ., data = Ozone)
beta_hat_lm = as.vector(omodel$coefficients)
```

Coefficients **with** using `lm()` `r beta_hat_lm`

`sum(beta_hat_lm ^ 2)` = `r sum(beta_hat_lm ^ 2)`

**(c)** Use the `all.equal()` function to verify that the results are the same. You may need to remove the names of one of the vectors. The `as.vector()` function will do this as a side effect, or you can directly use `unname()`.

**Answer:**

```{r}
all.equal(beta_hat_no_lm, beta_hat_lm)
```

**(d)** Calculate $s_e$ without the use of `lm()`. That is, continue with your results from **(a)** and perform additional matrix operations to obtain the result. Output this result. Also, verify that this result is the same as the result obtained from `lm()`.

**Answer:**

Calculating $s_e$ **without** using `lm()`

$s_e = \sqrt\frac{SSE}{df}$

$SSE = \sum_{i=1}^n(y_i - \hat y_i)^2$

$df = n - p$ Here $p$ is count of coefficients.

```{r}
estimate_no_lm = X %*% beta_hat_no_lm

# sum of squared residuals
SSE = sum((response - estimate_no_lm)^2)

# degrees of freedom of residuals
df = (nrow(Ozone) - length(beta_hat_no_lm))
se_no_lm = sqrt(SSE / df)
se_no_lm
```

Comparing $s_e$ obtained **without** `lm()` and **with** `lm()`

```{r}
summary(omodel)$sigma == se_no_lm
```

**(e)** Calculate $R^2$ without the use of `lm()`. That is, continue with your results from **(a)** and **(d)**, and perform additional operations to obtain the result. Output this result. Also, verify that this result is the same as the result obtained from `lm()`.

**Answer:**

$SST = SSE + SSE_{Reg}$

$R^2 = \frac{SSE_{Reg}}{SST} = 1 - \frac{SSE}{SST}$

```{r}
SST = sum((response -mean(response))^ 2)
r_sq = 1 - SSE/ SST
r_sq
```

Comparing with result using `lm()`

```{r}
# Note: `summary(omodel)$r.squared == r_sq` results in `FALSE` even though numbers are same.
# summary(omodel)$r.squared == r_sq
all.equal( summary(omodel)$r.squared, r_sq)
```

***

## Exercise 4 (Regression for Prediction)

For this exercise use the `Auto` dataset from the `ISLR` package. Use `?Auto` to learn about the dataset. The goal of this exercise is to find a model that is useful for **predicting** the response `mpg`. We remove the `name` variable as it is not useful for this analysis. (Also, this is an easier to load version of data from the textbook.)

```{r}
# load required package, remove "name" variable
library(ISLR)
Auto = subset(Auto, select = -c(name))
dim(Auto)
```

When evaluating a model for prediction, we often look at RMSE. However, if we both fit the model with all the data as well as evaluate RMSE using all the data, we're essentially cheating. We'd like to use RMSE as a measure of how well the model will predict on *unseen* data. If you haven't already noticed, the way we had been using RMSE resulted in RMSE decreasing as models became larger.

To correct for this, we will only use a portion of the data to fit the model, and then we will use leftover data to evaluate the model. We will call these datasets **train** (for fitting) and **test** (for evaluating). The definition of RMSE will stay the same

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

where

- $y_i$ are the actual values of the response for the given data.
- $\hat{y}_i$ are the predicted values using the fitted model and the predictors from the data.

However, we will now evaluate it on both the **train** set and the **test** set separately. So each model you fit will have a **train** RMSE and a **test** RMSE. When calculating **test** RMSE, the predicted values will be found by predicting the response using the **test** data with the model fit using the **train** data. *__Test__ data should never be used to fit a model.*

- Train RMSE: Model fit with *train* data. Evaluate on **train** data.
- Test RMSE: Model fit with *train* data. Evaluate on **test** data.

Set a seed of `22`, and then split the `Auto` data into two datasets, one called `auto_trn` and one called `auto_tst`. The `auto_trn` data frame should contain 290 randomly chosen observations. The `auto_tst` data will contain the remaining observations. Hint: consider the following code:

```{r}
set.seed(22)
auto_trn_idx = sample(1:nrow(Auto), 290)
```

Fit a total of five models using the training data.

- One must use all possible predictors.
- One must use only `displacement` as a predictor.
- The remaining three you can pick to be anything you like. One of these should be the *best* of the five for predicting the response.

For each model report the **train** and **test** RMSE. Arrange your results in a well-formatted markdown table. Argue that one of your models is the best for predicting the response.

**Answer**

Splitting `train` and `test` data.

```{r}
train_set = Auto[auto_trn_idx, ]
test_set = Auto[-auto_trn_idx, ]
```

  - Rows in `train_Set` = `r nrow(train_set)`
  - Rows in `test_Set` = `r nrow(test_set)`

We will be calculating `RMSE` a lot of times, a reusable function will be useful.

```{r}
rmse = function(data, fitted){
  residSq = (data - fitted)^2
  sqrt(mean(residSq))
}
```

Creating another function to automate calculation of train and test rmse.

```{r}
modelRmseData = function(model, test_data){
  train_predictors = subset(model$model, select = -c(mpg))
  train_actual_mpg = subset(model$model, select = c("mpg")
                            )
  fitted_train = predict(model, newdata = train_predictors)
  rmse_train = rmse(train_actual_mpg$mpg, fitted_train)
  
  test_predictors = subset(test_data, select = names(model$model)[-1])
  test_actual_mpg = subset(test_data, select = c("mpg"))
  
  fitted_test = predict(model, newdata = test_predictors)
  rmse_test = rmse(test_actual_mpg$mpg, fitted_test)
  
  cbind(RMSE_TRAIN = rmse_train, RMSE_TEST = rmse_test)
}
```

We will store all RMSE's in a data frame.

```{r}
rmsedf = data.frame()
```

- **Model 1:** All possible predictors

```{r}
model1 = lm(mpg ~ ., data = train_set)
rm = modelRmseData(model1, test_set)
rm

# Adding this data to data frame
rmsedf = rbind(rmsedf, cbind(MODEL="all", rm))
```

- **Model 2:** Only `displacement` predictors

```{r}
model2 = lm(mpg ~ displacement, data = train_set)

rm = modelRmseData(model2, subset(test_set, select = c("mpg", "displacement")))
rm

# Adding this data to data frame
rmsedf = rbind(rmsedf, cbind(MODEL="displacement", rm))
```

- **Model 3:** `cylinders, displacement, weight` predictors

```{r}
model3 = lm(mpg ~ cylinders + displacement + weight, data = train_set)

rm = modelRmseData(model3, subset(test_set, select = c("mpg", "cylinders", "displacement", "weight")))
rm

# Adding this data to data frame
rmsedf = rbind(rmsedf, cbind(MODEL="cylinders, displacement, weight", rm))
```

- **Model 4:** `cylinders, displacement, weight, year` predictors

```{r}
model4 = lm(mpg ~ cylinders + displacement + weight + year, data = train_set)

rm = modelRmseData(model4, subset(test_set, select = c("mpg", "cylinders", "displacement", "weight", "year")))
rm

# Adding this data to data frame
rmsedf = rbind(rmsedf, cbind(MODEL="cylinders, displacement, weight, year", rm))
```

- **Model 5:** `displacement, weight, year, origin` predictors

```{r}
model5 = lm(mpg ~ displacement + weight + year + origin, data = train_set)

rm = modelRmseData(model5, subset(test_set, select = c("mpg", "displacement", "weight", "year", "origin")))
rm

# Adding this data to data frame
rmsedf = rbind(rmsedf, cbind(MODEL="displacement, weight, year, origin", rm))
```

- Displaying result in table

```{r}
kable(rmsedf, align = "l")
```

- **Conclusion:** With this result, I would prefer model with predictors `displacement, weight, year, origin` because RMSE with these predictors is comparable with the one with all predictors but it uses much less variables.

***

## Exercise 5 (Simulating Multiple Regression)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 2$
- $\beta_1 = -0.75$
- $\beta_2 = 1.6$
- $\beta_3 = 0$
- $\beta_4 = 0$
- $\beta_5 = 2$
- $\sigma^2 = 25$

We will use samples of size `n = 40`.

We will verify the distribution of $\hat{\beta}_1$ as well as investigate some hypothesis tests.

**(a)** We will first generate the $X$ matrix and data frame that will be used throughout the exercise. Create the following nine variables:

- `x0`: a vector of length `n` that contains all `1`
- `x1`: a vector of length `n` that is randomly drawn from a normal distribution with a mean of `0` and a standard deviation of `2`
- `x2`: a vector of length `n` that is randomly drawn from a uniform distribution between `0` and `4`
- `x3`: a vector of length `n` that is randomly drawn from a normal distribution with a mean of `0` and a standard deviation of `1`
- `x4`: a vector of length `n` that is randomly drawn from a uniform distribution between `-2` and `2`
- `x5`: a vector of length `n` that is randomly drawn from a normal distribution with a mean of `0` and a standard deviation of `2`
- `X`: a matrix that contains `x0`, `x1`, `x2`, `x3`, `x4`, and `x5` as its columns
- `C`: the $C$ matrix that is defined as $(X^\top X)^{-1}$
- `y`: a vector of length `n` that contains all `0`
- `sim_data`: a data frame that stores `y` and the **five** *predictor* variables. `y` is currently a placeholder that we will update during the simulation.

Report the sum of the diagonal of `C` as well as the 5th row of `sim_data`. For this exercise we will use the seed `420`. Generate the above variables in the order listed after running the code below to set a seed.

```{r}
set.seed(400)
sample_size = 40
```

- Initializing variables

```{r}
n = 40
beta_0 = 2
beta_1 = -0.75
beta_2 = 1.6
beta_3 = 0
beta_4 = 0
beta_5 = 2
var = 25
std = sqrt(var)
```

- Initializing data

```{r}
x0 = rep(1, n)
x1 = rnorm(n, mean = 0, sd = 2)
x2 = runif(n, min = 0, max = 4)
x3 = rnorm(n, mean = 0, sd = 1)
x4 = runif(n, min = -2, max = 2)
x5 = rnorm(n, mean = 0, sd = 2)
X = data.matrix(data.frame(x0, x1, x2, x3, x4, x5))
C = solve(t(X) %*% X)
y = rep(0, n)
sim_data = data.frame(y, x1, x2, x3, x4, x5)
```

- Reporting sum of diagonals of C

```{r}
sum(diag(C))
```

- Reporting 5th row of sim_data

```{r}
sim_data[5, ]
```

**(b)** Create three vectors of length `2500` that will store results from the simulation in part **(c)**. Call them `beta_hat_1`, `beta_3_pval`, and `beta_5_pval`.

- Creating vectors with all values as NA

```{r}
beta_hat_1 = rep(NA, 2500)
beta_3_pval = rep(NA, 2500)
beta_5_pval = rep(NA, 2500)
```

**(c)** Simulate 2500 samples of size `n = 40` from the model above. Each time update the `y` value of `sim_data`. Then use `lm()` to fit a multiple regression model. Each time store:

- The value of $\hat{\beta}_1$ in `beta_hat_1`
- The p-value for the two-sided test of $\beta_3 = 0$ in `beta_3_pval`
- The p-value for the two-sided test of $\beta_5 = 0$ in `beta_5_pval`

**Answer:**

- Creating a function to help with simulation.

```{r}
sim_func = function(n){
  eps = rnorm(n, mean = 0, sd = std)
  X %*% c(beta_0, beta_1, beta_2, beta_3, beta_4, beta_5) + eps
}
```

- Simulating `2500` times and updating `beta_hat_1`, `beta_3_pval`, and `beta_5_pval`.

```{r}
for (iter in 1:2500) {
  sim_data$y = sim_func(n)
  model = lm(y ~ x1 + x2 + x3 + x4 + x5, data = sim_data)
  beta_hat_1[iter] = coef(model)["x1"]
  beta_3_pval[iter] = summary(model)$coefficients["x3", "Pr(>|t|)"]
  beta_5_pval[iter] = summary(model)$coefficients["x5", "Pr(>|t|)"]
}
```

**(d)** Based on the known values of $X$, what is the true distribution of $\hat{\beta}_1$?

**Answer:**

  - mean = $\beta_1$
  - std deviation = $\sqrt{\sigma^2 * C22}$

- **Result:** True distribution of $\hat{\beta}_1$ should be $N(\mu = \beta_1, \sigma^2 = \sigma^2 * C22)$. i.e. N(`r beta_1`, `r var * C[2,2]`).

**(e)** Calculate the mean and variance of `beta_hat_1`. Are they close to what we would expect? Plot a histogram of `beta_hat_1`. Add a curve for the true distribution of $\hat{\beta}_1$. Does the curve seem to match the histogram?

**Answer:**

This time we will plot histogram of $\hat{\beta}_1$ and fit curve based on mean and std of estimated $\hat\beta_1$ on that to check if this is normal distribution.

```{r}
mean(beta_hat_1)
var(beta_hat_1)
```

Yes, these values are close to what I expected (real mean and variance)

```{r}
hist(beta_hat_1, main = "", prob = TRUE, xlab = expression(hat(beta)[1]))
curve(dnorm(x, mean = beta_1, sd =  sqrt(var * C[2,2])), col = "darkorange", add = TRUE, lwd = 3)
```

**Result:** This curve fits really well with histogram.

**(f)** What proportion of the p-values stored in `beta_3_pval` is less than 0.10? Is this what you would expect?

**Answer:** 

```{r}
mean(beta_3_pval < 0.1)
```

Proportion of p-values less than 0.1 is **`r mean(beta_3_pval < 0.1)`**

This means proportion of simulations in which we can reject null hypothesis $H_0: \beta_3 = 0$ for $\alpha = 0.1$ is `r mean(beta_3_pval < 0.1)`

**This makes sense: Since we know that in actual model $\beta_3 = 0$. I expected that we will Fail to reject null hypothesis most times. Which is evident by this result.**


**(g)** What proportion of the p-values stored in `beta_5_pval` is less than 0.01? Is this what you would expect?

**Answer:** 

```{r}
mean(beta_5_pval < 0.01)
```

Proportion of p-values less than 0.01 is **`r mean(beta_5_pval < 0.01)`**

This means proportion of simulations in which we can reject null hypothesis $H_0: \beta_5 = 0$ for $\alpha = 0.01$ is `r mean(beta_5_pval < 0.01)`

Yes, I expected this result.

**Since we know that in actual model $\beta_5 \neq 0$. Which means, we should have rejected null hypothesis most of the times. Which is clearly visible by the result.**